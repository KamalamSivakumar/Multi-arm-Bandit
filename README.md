# Multi-arm-Bandit
Using Python, different methods in the Multi-Arm-Bandit problem are seen.

The multi-arm bandit problem is a classic exploration-exploitation dilemma in the field of reinforcement learning and decision-making under uncertainty. 
It's named after the scenario where a gambler faces a row of slot machines (or "one-armed bandits") and must decide which machine to play (or "pull") to maximize their total reward over time.

The goal is to balance the exploration of uncertain actions to learn about their reward distributions (exploration) and the exploitation of actions that are believed to yield high rewards based on current knowledge (exploitation).

Different Approaches:
  1. Greedy
  2. Epsilon Greedy
  3. Decayed Epsilon
  4. Upper Confidence Bound
  5. Incremental Uniform
